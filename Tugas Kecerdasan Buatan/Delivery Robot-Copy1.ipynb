{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6264ad1c",
   "metadata": {},
   "source": [
    "<h1 style=\"line-height:.1;\"><center>Tugas Akhir Kecerdasan Buatan TIF-A SA</center></h1>\n",
    "<h1 style=\"line-height:.5;\"><center>Catatan Pengerjaan Program</center></h1>\n",
    "\n",
    "---\n",
    "Dimas Tri Mustakim (205150200111049)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d7b9cb",
   "metadata": {},
   "source": [
    "# Kode Lingkungan / Environment\n",
    "\n",
    "Peta lingkungan dibuat berdasarkan kode program dari salah satu lingkungan yang tersedia di library OpenAI Gym yang bernama Taxi. Disini kami menggunakan kode program tersebut dan memodifikasinya untuk memenuhi topik yang kami pilih yaitu tentang robot pengangkut barang. Berikut beberapa hal yang saya ubah.\n",
    "\n",
    "1. Merubah ukuran lingkungan menjadi 10x10 dari semula 4x4\n",
    "2. Merubah letak goal\n",
    "3. Merubah model reward\n",
    "4. Sedikit modifikasi pewarnaan lingkungan saat render\n",
    "5. Merubah beberapa parameter\n",
    "6. Membuat method step() untuk assert nilai masukan untuk memastikan nilai masukan \n",
    "7. Merubah nama variable untuk menggambarkan lingkungan robot pengangkut barang\n",
    "\n",
    "Link source dari lingkungan kami ada disini : [Taxi milik OpenAI Gym](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py).\n",
    "\n",
    "Source code tersebut dirilis secara open source dengan lisensi MIT dan dapat dilihat di link berikut ini\n",
    "* https://github.com/openai/gym/blob/master/LICENSE.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb68ca1",
   "metadata": {},
   "source": [
    "##### Highlight perubahan kode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47738920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peta lingkungan/environment\n",
    "# Karakter '|' dan ':' dapat direplace satu sama lain untuk memodifikasi\n",
    "\n",
    "MAP = [\n",
    "    \"+-------------------+\",\n",
    "    \"|R| | : | : | : : :G|\",\n",
    "    \"| | | | : : | : : : |\",\n",
    "    \"| | : | : : | : | : |\",\n",
    "    \"| | : | : : | : | : |\",\n",
    "    \"| | : | : : | : | : |\",\n",
    "    \"| | : | : |B| : | : |\",\n",
    "    \"| : : | : : : : | : |\",\n",
    "    \"| | : | : : : : | : |\",\n",
    "    \"| | : | : : : : | : |\",\n",
    "    \"|Y| : : : : | : : : |\",\n",
    "    \"+-------------------+\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91131d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeliveryRobotEnv(discrete.DiscreteEnv):\n",
    "    \n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.desc = np.asarray(MAP, dtype='c')\n",
    "\n",
    "        self.locs = locs = [(0, 0), (0, 9), (9, 0), (5, 5)]\n",
    "\n",
    "        num_states = 2000  # 10 * 10 * 5 * 4\n",
    "        num_rows = 10\n",
    "        num_columns = 10\n",
    "        max_row = num_rows - 1\n",
    "        max_col = num_columns - 1\n",
    "        initial_state_distrib = np.zeros(num_states)\n",
    "        num_actions = 6\n",
    "        P = {state: {action: [] for action in range(num_actions)} for state in range(num_states)}\n",
    "\n",
    "        for row in range(num_rows):\n",
    "            for col in range(num_columns):\n",
    "                for goods_idx in range(len(locs) + 1):\n",
    "                    for dest_idx in range(len(locs)):\n",
    "                        state = self.encode(row, col, goods_idx, dest_idx)\n",
    "                        if goods_idx < 4 and goods_idx != dest_idx:\n",
    "                            initial_state_distrib[state] += 1\n",
    "                        for action in range(num_actions):\n",
    "                            new_row, new_col, new_goods_idx = row, col, goods_idx\n",
    "                            reward = -1 \n",
    "                            done = False\n",
    "                            robot_loc = (row, col)\n",
    "\n",
    "                            if action == 0:\n",
    "                                new_row = min(row + 1, max_row)\n",
    "                            elif action == 1:\n",
    "                                new_row = max(row - 1, 0)\n",
    "                            if action == 2 and self.desc[1 + row, 2 * col + 2] == b\":\":\n",
    "                                new_col = min(col + 1, max_col)\n",
    "                            elif action == 3 and self.desc[1 + row, 2 * col] == b\":\":\n",
    "                                new_col = max(col - 1, 0)\n",
    "                            elif action == 4:  # pickup\n",
    "                                if goods_idx < 4 and robot_loc == locs[goods_idx]:\n",
    "                                    new_goods_idx = 4\n",
    "                                else:  # goods not at location\n",
    "                                    reward = -10\n",
    "                            elif action == 5:  # dropoff\n",
    "                                if (robot_loc == locs[dest_idx]) and goods_idx == 4:\n",
    "                                    new_goods_idx = dest_idx\n",
    "                                    done = True\n",
    "                                    reward = 30\n",
    "                                elif (robot_loc in locs) and goods_idx == 4:\n",
    "                                    new_goods_idx = locs.index(robot_loc)\n",
    "                                else:  # dropoff at wrong location\n",
    "                                    reward = -10\n",
    "                            new_state = self.encode(new_row, new_col, new_goods_idx, dest_idx)\n",
    "                            P[state][action].append((1.0, new_state, reward, done))\n",
    "\n",
    "        initial_state_distrib /= initial_state_distrib.sum()\n",
    "        discrete.DiscreteEnv.__init__(self, num_states, num_actions, P, initial_state_distrib)\n",
    "\n",
    "    def encode(self, robot_row, robot_col, goods_loc, dest_idx):\n",
    "        \"\"\"\n",
    "        Berfungsi untuk menencode keadaan lingkungan yang berupa (robot_row, robot_col, lokasi barang, tujuan)\n",
    "        ke dalam suatu angka diskrit untuk merepresentasikan keadaan lingkungan\n",
    "        \n",
    "        4 data keadaan lingkungan tersebut akan di encode ke dalam nilai diskrit di antara 0 sampai 2000\n",
    "        \n",
    "        \"\"\"\n",
    "        i = robot_row\n",
    "        i *= 10\n",
    "        i += robot_col\n",
    "        i *= 5\n",
    "        i += goods_loc\n",
    "        i *= 4\n",
    "        i += dest_idx\n",
    "        return i\n",
    "\n",
    "    def decode(self, i):\n",
    "        \"\"\"\n",
    "        Untuk men-decode angka representasi state kembali ke bentuk (robot_row, robot_col, lokasi_barang, tujuan)\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        out.append(i % 4)\n",
    "        i = i // 4\n",
    "        out.append(i % 5)\n",
    "        i = i // 5\n",
    "        out.append(i % 10)\n",
    "        i = i // 10\n",
    "        out.append(i)\n",
    "        assert 0 <= i < 10\n",
    "        return reversed(out)\n",
    "\n",
    "    def step(self, a):\n",
    "        assert 0 <= a < 2000\n",
    "        return super().step(a)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "        out = self.desc.copy().tolist()\n",
    "        out = [[c.decode('utf-8') for c in line] for line in out]\n",
    "        robot_row, robot_col, goods_idx, dest_idx = self.decode(self.s)\n",
    "\n",
    "        # Coloring barrier ( '|', '+', '-' )\n",
    "        for a in out:\n",
    "            for b in range(len(a)):\n",
    "                if a[b] == '|' or a[b] == '+' or a[b] == '-':\n",
    "                    a[b] = utils.colorize(a[b], 'cyan')\n",
    "\n",
    "        def ul(x):\n",
    "            return \"_\" if x == \" \" else x\n",
    "\n",
    "        if goods_idx < 4:\n",
    "            out[1 + robot_row][2 * robot_col + 1] = utils.colorize(\n",
    "                out[1 + robot_row][2 * robot_col + 1], 'yellow', highlight=True)\n",
    "            pi, pj = self.locs[goods_idx]\n",
    "            out[1 + pi][2 * pj + 1] = utils.colorize(out[1 + pi][2 * pj + 1], 'blue', bold=True)\n",
    "        else:  # passenger in taxi\n",
    "            out[1 + robot_row][2 * robot_col + 1] = utils.colorize(\n",
    "                ul(out[1 + robot_row][2 * robot_col + 1]), 'green', highlight=True)\n",
    "\n",
    "        di, dj = self.locs[dest_idx]\n",
    "        out[1 + di][2 * dj + 1] = utils.colorize(out[1 + di][2 * dj + 1], 'magenta')\n",
    "        outfile.write(\"\\n\".join([\"\".join(row) for row in out]) + \"\\n\")\n",
    "        if self.lastaction is not None:\n",
    "            outfile.write(\"  ({})\\n\".format(\n",
    "                [\"Selatan\", \"Utara\", \"Timur\", \"Barat\", \"Ambil Barang\", \"Taruh Barang\"][self.lastaction])\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "        if mode != 'human':\n",
    "            with closing(outfile):\n",
    "                return outfile.getvalue()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f958f0b",
   "metadata": {},
   "source": [
    "# Penggunaan lingkungan\n",
    "looping episode, cara interaksi, penggunaan method dan berbagai parameter mengikuti standar yang ditetapkan oleh OpenAI Gym kepada lingkungan mereka. Jadi disini kode untuk penggunaan lingkungan kami ini mengikuti petunjuk yang telah ditetapkan pada dokumentasi library milik mereka yang bisa diakses **[disini](https://gym.openai.com/docs/)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9354fb",
   "metadata": {},
   "source": [
    "# Kode Q-Learning\n",
    "\n",
    "Kode untuk Q-Learning (q-table, parameter yang digunakan, rumus dan kode untuk update q-value) saya pelajari dari beberapa sumber di internet (youtube, blog, medium) dan tutorial reinforcement learning di lingkungan Taxi dari gym karena lingkungannya terinspirasi dari lingkungan tersebut. Sehingga inti dari program ini mengikuti tutorial tersebut dengan beberapa perubahan.\n",
    "\n",
    "Berikut beberapa resource yang saya gunakan :\n",
    "1. https://towardsdatascience.com/simple-reinforcement-learning-q-learning-fcddc4b6fe56\n",
    "2. https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee2f44f",
   "metadata": {},
   "source": [
    "Selebihnya dari kode kecil2 yang saya buat di notebook program merupakan buatan saya sendiri."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae9d34c",
   "metadata": {},
   "source": [
    "# Kesimpulan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7838c307",
   "metadata": {},
   "source": [
    "Disini saya membuat program ini dengan tujuan awal mulai mempelajari ai, reinforcement learning dan dalam pengerjaannya menggunakan metode Amati, Tiru, Modifikasi. Sehingga saya merasa makalah tugas ini kurang cocok dimasukkan ke dalam jurnal dsb. Tetapi saya nanti mungkin masukkan ke dalam blog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7036145",
   "metadata": {},
   "source": [
    "## Terimakasih, mohon maaf yang sebesar-besarnya jika ada kesalahan yang pernah saya perbuat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6264ad1c",
   "metadata": {},
   "source": [
    "<h1 style=\"line-height:.1;\"><center>Tugas Akhir Kecerdasan Buatan TIF-A SA</center></h1>\n",
    "<h1 style=\"line-height:.5;\"><center>Robot Pengangkut Barang Otomatis Berbasis Q-Learning</center></h1>\n",
    "\n",
    "---\n",
    "\n",
    "<p1 style=\"line-height:5;\"><center>Oleh Kelompok 2 :</center></p1>\n",
    "<p1 style=\"line-height:5;\"><center>Dimas Tri Mustakim (205150200111049)</center></p1>\n",
    "<p1 style=\"line-height:5;\"><center>Salsabila Dita Prasetya (205150201111024)</center></p1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99151ada",
   "metadata": {},
   "source": [
    "# Latar Belakang\n",
    "\n",
    "Sekarang ini kebutuhan untuk transportasi barang merupakan hal yang tidak kalah pentingnya di dalam dunia industri. Transportasi disini tidak hanya dalam jarak jauh saja, tetapi kebutuhan untuk jarak dekat. Di dalam suatu perusahaan seperti perusahaan manufaktur, arus perpindahan barang terjadi sangat cepat. Barang atau bahan produksi perlu dipindahkan setiap saat ke proses berikutnya untuk membuat suatu produk. Sekarang proses tersebut masih banyak yang melibatkan manusia di dalamnya, tetapi hal itu tidak bisa dilakukan untuk barang-barang yang sangat berat, barang berbahaya (racun, radiasi, dll) dan barang yang mudah rusak jika terkontaminasi. Karena hal itulah kita perlu membuat cara baru untuk transportasi barang, salah satunya adalah dengan menggunakan robot dengan Artificial Intelligence di dalamnya."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe65c3c",
   "metadata": {},
   "source": [
    "### Contoh aplikasi agen pengangkut barang\n",
    "##### 1. Robot warehouse\n",
    "\n",
    "<img src=\"https://cdn.vox-cdn.com/thumbor/FnXcyoYv1x5muxd1fY-MdkvnJAI=/0x0:3000x2000/2070x1164/filters:focal(1204x757:1684x1237):format(webp)/cdn.vox-cdn.com/uploads/chorus_image/image/59668735/ocado_robot_smaller.0.png\" alt=\"robot warehouse\" width=\"700\"/>\n",
    "\n",
    "##### 2. Robot transportasi intralogistik\n",
    "\n",
    "<img src=\"https://software.intel.com/content/dam/develop/external/us/en/images/seit-amr-from-milvus-robotics-749347.png\" alt=\"intralogistic robot\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e583075",
   "metadata": {},
   "source": [
    "# Program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b441ae",
   "metadata": {},
   "source": [
    "## Membuat Lingkungan\n",
    "\n",
    "Lingkungan ini dibuat berdasarkan lingkungan dari OpenAI Gym yakni \"Taxi-v3\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ca307",
   "metadata": {},
   "source": [
    "### Deskripsi lingkungan:\n",
    "\n",
    "Terdapat lingkungan berbentuk persegi (grid) sebesar 10x10. Di lingkungan tersebut terdapat 4 buah lokasi yang ditandakan dengan huruf R, G, Y, dan B. Barang akan muncul secara random di salah satu tempat tersebut dan tujuan barang juga di salah satu tempat tersebut. Ketika episode dimulai, robot akan mulai di posisi acak di dalam grid. Robot bertugas untuk menjemput barang, mengambil, dan kemudian mengantarkannya ke tempat tujuan. Dalam berjalan, robot tidak dapat melewati garis '|'. Episode berakhir ketika barang telah selesai diantarkan.\n",
    "    \n",
    "### Observasi :\n",
    "\n",
    "Terdapat 2000 keadaan diskrit yang didapat dari 100 kemungkinan posisi taxi (10 x 10), 5 lokasi yang mungkin untuk barang, dan 4 tujuan pengiriman barang.\n",
    "    \n",
    "Lokasi barang :\n",
    "- 0: R\n",
    "- 1: G\n",
    "- 2: Y\n",
    "- 3: B\n",
    "- 4: Di bawa robot\n",
    "    \n",
    "Tujuan antar:\n",
    "- 0: R\n",
    "- 1: G\n",
    "- 2: Y\n",
    "- 3: B\n",
    "    \n",
    "### Actions:\n",
    "\n",
    "Terdapat 6 aksi yang bersifat deterministic yang dapat diambil oleh agen\n",
    "    \n",
    "- 0: jalan selatan\n",
    "- 1: jalan ke utara\n",
    "- 2: jalan ke timur\n",
    "- 3: jalan ke barat\n",
    "- 4: ambil barang\n",
    "- 5: taruh barang\n",
    "\n",
    "### Rewards\n",
    "\n",
    "1. Agen akan dihadiahi sejumlah -1 setiap step/langkah yang dilakukan. Kecuali :\n",
    "2. Ketika barang telah dikirimkan ke tempatnya akan dihadiahi poin 30.\n",
    "3. Ketika melakukan aksi pickup atau drop-off secara illegal, akan dikenai poin -10\n",
    "    \n",
    "    \n",
    "### Rendering:\n",
    "\n",
    "- blue: barang yang hendak diambil\n",
    "- magenta: tujuan barang\n",
    "- yellow: robot yang belum membawa barang\n",
    "- green: robot yang sedang membawa barang\n",
    "- cyan: barrier atau elemen yang tidak bisa dilewati\n",
    "- other letters (R, G, Y and B): lokasi dari barang dan tujuannya\n",
    "    \n",
    "### Keadaan lingkungan direpresentasikan dengan format\n",
    "\n",
    "(robot_row, robot_col, lokasi_barang, tujuan) yang nantinya akan di encode ke dalam satu nilai diskrit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62561494",
   "metadata": {},
   "source": [
    "### Import library yang dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76d7287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, random\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from contextlib import closing\n",
    "from io import StringIO\n",
    "\n",
    "from gym import utils\n",
    "from gym.envs.toy_text import discrete\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d7b9cb",
   "metadata": {},
   "source": [
    "### Peta lingkungan tempat robot berinteraksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47738920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peta lingkungan/environment\n",
    "# Karakter '|' dan ':' dapat direplace satu sama lain untuk memodifikasi\n",
    "\n",
    "MAP = [\n",
    "    \"+-------------------+\",\n",
    "    \"|R| | : | : | : : :G|\",\n",
    "    \"| | | | : : | : : : |\",\n",
    "    \"| | : | : : | : | : |\",\n",
    "    \"| | : | : : | : | : |\",\n",
    "    \"| | : | : : | : | : |\",\n",
    "    \"| | : | : |B| : | : |\",\n",
    "    \"| : : | : : : : | : |\",\n",
    "    \"| | : | : : : : | : |\",\n",
    "    \"| | : | : : : : | : |\",\n",
    "    \"|Y| : : : : | : : : |\",\n",
    "    \"+-------------------+\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07f144b",
   "metadata": {},
   "source": [
    "### Membuat lingkungan\n",
    "\n",
    "Kode merupakan hasil modifikasi dari lingkungan [Taxi milik OpenAI Gym](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py).\n",
    "\n",
    "License dari OpenAI Gym : [license](https://github.com/openai/gym/blob/master/LICENSE.md)\n",
    "\n",
    "Kelas untuk robot ini kami namai **DeliveryRobotEnv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91131d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeliveryRobotEnv(discrete.DiscreteEnv):\n",
    "    \n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.desc = np.asarray(MAP, dtype='c')\n",
    "\n",
    "        self.locs = locs = [(0, 0), (0, 9), (9, 0), (5, 5)]\n",
    "\n",
    "        num_states = 2000  # 10 * 10 * 5 * 4\n",
    "        num_rows = 10\n",
    "        num_columns = 10\n",
    "        max_row = num_rows - 1\n",
    "        max_col = num_columns - 1\n",
    "        initial_state_distrib = np.zeros(num_states)\n",
    "        num_actions = 6\n",
    "        P = {state: {action: [] for action in range(num_actions)} for state in range(num_states)}\n",
    "\n",
    "        for row in range(num_rows):\n",
    "            for col in range(num_columns):\n",
    "                for goods_idx in range(len(locs) + 1):\n",
    "                    for dest_idx in range(len(locs)):\n",
    "                        state = self.encode(row, col, goods_idx, dest_idx)\n",
    "                        if goods_idx < 4 and goods_idx != dest_idx:\n",
    "                            initial_state_distrib[state] += 1\n",
    "                        for action in range(num_actions):\n",
    "                            new_row, new_col, new_goods_idx = row, col, goods_idx\n",
    "                            reward = -1 \n",
    "                            done = False\n",
    "                            robot_loc = (row, col)\n",
    "\n",
    "                            if action == 0:\n",
    "                                new_row = min(row + 1, max_row)\n",
    "                            elif action == 1:\n",
    "                                new_row = max(row - 1, 0)\n",
    "                            if action == 2 and self.desc[1 + row, 2 * col + 2] == b\":\":\n",
    "                                new_col = min(col + 1, max_col)\n",
    "                            elif action == 3 and self.desc[1 + row, 2 * col] == b\":\":\n",
    "                                new_col = max(col - 1, 0)\n",
    "                            elif action == 4:  # pickup\n",
    "                                if goods_idx < 4 and robot_loc == locs[goods_idx]:\n",
    "                                    new_goods_idx = 4\n",
    "                                else:  # goods not at location\n",
    "                                    reward = -10\n",
    "                            elif action == 5:  # dropoff\n",
    "                                if (robot_loc == locs[dest_idx]) and goods_idx == 4:\n",
    "                                    new_goods_idx = dest_idx\n",
    "                                    done = True\n",
    "                                    reward = 30\n",
    "                                elif (robot_loc in locs) and goods_idx == 4:\n",
    "                                    new_goods_idx = locs.index(robot_loc)\n",
    "                                else:  # dropoff at wrong location\n",
    "                                    reward = -10\n",
    "                            new_state = self.encode(new_row, new_col, new_goods_idx, dest_idx)\n",
    "                            P[state][action].append((1.0, new_state, reward, done))\n",
    "\n",
    "        initial_state_distrib /= initial_state_distrib.sum()\n",
    "        discrete.DiscreteEnv.__init__(self, num_states, num_actions, P, initial_state_distrib)\n",
    "\n",
    "    def encode(self, robot_row, robot_col, goods_loc, dest_idx):\n",
    "        \"\"\"\n",
    "        Berfungsi untuk menencode keadaan lingkungan yang berupa (robot_row, robot_col, lokasi barang, tujuan)\n",
    "        ke dalam suatu angka diskrit untuk merepresentasikan keadaan lingkungan\n",
    "        \n",
    "        4 data keadaan lingkungan tersebut akan di encode ke dalam nilai diskrit di antara 0 sampai 2000\n",
    "        \n",
    "        \"\"\"\n",
    "        i = robot_row\n",
    "        i *= 10\n",
    "        i += robot_col\n",
    "        i *= 5\n",
    "        i += goods_loc\n",
    "        i *= 4\n",
    "        i += dest_idx\n",
    "        return i\n",
    "\n",
    "    def decode(self, i):\n",
    "        \"\"\"\n",
    "        Untuk men-decode angka representasi state kembali ke bentuk (robot_row, robot_col, lokasi_barang, tujuan)\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        out.append(i % 4)\n",
    "        i = i // 4\n",
    "        out.append(i % 5)\n",
    "        i = i // 5\n",
    "        out.append(i % 10)\n",
    "        i = i // 10\n",
    "        out.append(i)\n",
    "        assert 0 <= i < 10\n",
    "        return reversed(out)\n",
    "\n",
    "    def step(self, a):\n",
    "        assert 0 <= a < 2000\n",
    "        return super().step(a)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "        out = self.desc.copy().tolist()\n",
    "        out = [[c.decode('utf-8') for c in line] for line in out]\n",
    "        robot_row, robot_col, goods_idx, dest_idx = self.decode(self.s)\n",
    "\n",
    "        # Coloring barrier ( '|', '+', '-' )\n",
    "        for a in out:\n",
    "            for b in range(len(a)):\n",
    "                if a[b] == '|' or a[b] == '+' or a[b] == '-':\n",
    "                    a[b] = utils.colorize(a[b], 'cyan')\n",
    "\n",
    "        def ul(x):\n",
    "            return \"_\" if x == \" \" else x\n",
    "\n",
    "        if goods_idx < 4:\n",
    "            out[1 + robot_row][2 * robot_col + 1] = utils.colorize(\n",
    "                out[1 + robot_row][2 * robot_col + 1], 'yellow', highlight=True)\n",
    "            pi, pj = self.locs[goods_idx]\n",
    "            out[1 + pi][2 * pj + 1] = utils.colorize(out[1 + pi][2 * pj + 1], 'blue', bold=True)\n",
    "        else:  # passenger in taxi\n",
    "            out[1 + robot_row][2 * robot_col + 1] = utils.colorize(\n",
    "                ul(out[1 + robot_row][2 * robot_col + 1]), 'green', highlight=True)\n",
    "\n",
    "        di, dj = self.locs[dest_idx]\n",
    "        out[1 + di][2 * dj + 1] = utils.colorize(out[1 + di][2 * dj + 1], 'magenta')\n",
    "        outfile.write(\"\\n\".join([\"\".join(row) for row in out]) + \"\\n\")\n",
    "        if self.lastaction is not None:\n",
    "            outfile.write(\"  ({})\\n\".format(\n",
    "                [\"Selatan\", \"Utara\", \"Timur\", \"Barat\", \"Ambil Barang\", \"Taruh Barang\"][self.lastaction])\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "        if mode != 'human':\n",
    "            with closing(outfile):\n",
    "                return outfile.getvalue()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4190ccc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space :  Discrete(2000)\n",
      "Action space :  Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "env = DeliveryRobotEnv()\n",
    "print(\"Observation space : \", env.observation_space)\n",
    "print(\"Action space : \", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249a4545",
   "metadata": {},
   "source": [
    ">Disini lingkungan memiliki tipe **Observation Space** dan **Action Space** diskrit. \n",
    ">\n",
    ">Dengan kata lain, lingkungan akan mengembalikan nilai diskrit ke agen sebagai nilai yang merepresentasikan keadaan lingkungan. Di program ini terdapat 2000 (0-1999) nilai observation space. Agen membuat nilai observation space in dengan menggunakan method encode.\n",
    ">\n",
    ">Agen kemudian dapat merespon dengan aksi berupa nilai diskrit 0 sampai 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "496b2d49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1111"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mencoba untuk men encode suatu keadaan berikut: \n",
    "\n",
    "# robot_row : 5\n",
    "# robot_col : 5\n",
    "# lokasi barang : 2 \n",
    "# tujuan : 3\n",
    "\n",
    "env.encode(5,5,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94f09afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posisi_robot_row : 5\n",
      "posisi_robot_col : 5\n",
      "lokasi barang : 2\n",
      "tujuan : 3\n"
     ]
    }
   ],
   "source": [
    "# sekarang kita coba decode kembali\n",
    "robot_row, robot_col, lokasi_barang, tujuan = env.decode(1111)\n",
    "print(f\"posisi_robot_row : {robot_row}\\nposisi_robot_col : {robot_col}\\nlokasi barang : {lokasi_barang}\\ntujuan : {tujuan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c227b937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m+\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m+\u001B[0m\n",
      "\u001B[36m|\u001B[0mR\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : :G\u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m \u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : : \u001B[36m|\u001B[0m : : : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\u001B[35m\u001B[43mB\u001B[0m\u001B[0m\u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m : : \u001B[36m|\u001B[0m : : : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m\u001B[34;1mY\u001B[0m\u001B[36m|\u001B[0m : : : : \u001B[36m|\u001B[0m : : : \u001B[36m|\u001B[0m\n",
      "\u001B[36m+\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m+\u001B[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kita coba render tampilan keadaan lingkungan seperti keadaan yang dimaksud\n",
    "env.s = env.encode(5,5,2,3)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a4ab39",
   "metadata": {},
   "source": [
    "### Mengetes lingkungan yang dibuat dengan random action\n",
    "\n",
    "Disini kita mencoba lingkungan yang telah dibuat menggunakan agen yang memutuskan langkahnya secara random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22d7c7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ke : 3\n",
      "Observatian space / State : 627\n",
      "Number of Steps : 49\n",
      "\u001B[36m+\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m+\u001B[0m\n",
      "\u001B[36m|\u001B[0mR\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : :\u001B[34;1mG\u001B[0m\u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m \u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : : \u001B[36m|\u001B[0m : : : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m\u001B[43m \u001B[0m: \u001B[36m|\u001B[0m : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\u001B[35mB\u001B[0m\u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m : : \u001B[36m|\u001B[0m : : : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0mY\u001B[36m|\u001B[0m : : : : \u001B[36m|\u001B[0m : : : \u001B[36m|\u001B[0m\n",
      "\u001B[36m+\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m+\u001B[0m\n",
      "  (Barat)\n",
      "\n",
      "Result\n",
      "Episode : 3\n",
      "Score : -185\n",
      "Status : Belum Sukses\n"
     ]
    }
   ],
   "source": [
    "# Mengetes lingkungan yang telah dibuat\n",
    "\n",
    "jumlah_episode = 3\n",
    "max_step = 50\n",
    "\n",
    "for episode in range(jumlah_episode):\n",
    "    obs = env.reset()\n",
    "    score = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    while not done and step != max_step:\n",
    "        \n",
    "        action = env.action_space.sample()\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print(f\"Episode ke : {episode+1}\")\n",
    "        print(f\"Observatian space / State : {obs}\")\n",
    "        print(f\"Number of Steps : {step}\")\n",
    "        \n",
    "        env.render()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        sleep(0.15)\n",
    "        score += reward\n",
    "        step += 1\n",
    "              \n",
    "    print(f\"\\nResult\\nEpisode : {episode+1}\\nScore : {score}\")\n",
    "    status = \"Sukses\" if done else \"Belum Sukses\"\n",
    "    print(f\"Status : {status}\")\n",
    "    sleep(2)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9354fb",
   "metadata": {},
   "source": [
    "# Membuat Agen Cerdas menggunakan Q-Learning\n",
    "\n",
    "**Q-Learning**  merupakan salah satu algoritma reinforcement learning yang berusaha untuk menemukan tindakan terbaik untuk dilakukan dalam suatu keadaan. Q-Learning berusaha mempelajari kebijakan yang dapat memaksimalkan reward/hadiah.\n",
    "\n",
    "Disini q-learning melakukannya dengan menggunakan Q-table. Q table merupakan matrix yang memiliki bentuk ``states x actions``.\n",
    "\n",
    "Saat training agen akan berinteraksi dengan lingkungannya berdasarkan dua cara, yaitu dengan \"exploiting\" dan \"exploring\". Exploiting berarti memanfaatkan informasi langkah yang paling baik diambil yang tersimpan di q-table untuk melakukan suatu langkah. Sedangkan dengan exploring agen akan mengambil langkah secara acak, hal ini dapat memungkinkan agen untuk menemukan langkah baru yang mungkin lebih baik dan menyimpannya di dalam q-table. Kita disini dapat menyeimbangkan exploitation/exploration dengan menggunakan parameter epsilon.\n",
    "\n",
    "Di saat training, agen akan memperbarui q-table pada setiap langkah berdasarkan reward yang didapatkannya hingga episode tersebut berakhir. Proses ini akan dilakukan berulang-ulang hingga didapatkan nilai q (q-values) yang optimal. Nilai dari q-values dihitung menggunakan rumus matematika seperti berikut :\n",
    "\n",
    "![rumus q](./rumusqlearning.png)\n",
    "\n",
    "Disini ada beberapa parameter yang berperan dalam mengupdate q-values, yakni\n",
    "* **Epsilon (ϵ)**, menyatakan seberapa banyak exploration yang ingin dilakukan\n",
    "* **Learning rate atau alpha (α)**, yaitu seberapa banyak nilai baru akan diterima dibanding nilai lama.\n",
    "* **Gamma (γ)**, untuk menyeimbangkan reward yang akan didapatkan dalam jangka pendek dan dalam jangka panjang.\n",
    "* **Reward**, merupakan nilai yang diberikan ketika telah menyelesaikan suatu aksi.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/lds-media/images/q-matrix-initialized-to-learned_gQq0BFs.width-1200.png\" alt=\"intralogistic robot\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e93a2786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat Q Table\n",
    "# dibuat dengan menggunakan numpy array dengan default value 0\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72444bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengatur hyperparameter untuk mengupdate q-table\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f73410d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.\n",
      "Episode: 900/1000\n",
      "\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Melakukan training sebanyak total_episodes\n",
    "total_episodes = 1000\n",
    "\n",
    "for i in range(1, total_episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:  \n",
    "            action = env.action_space.sample()  # Melakukan eksplorasi/exploration\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Melakukan exploitasi/exploitation\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)  # Memberikan aksi ke lingkungan\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)  # Menghitung nilai baru untuk q-table\n",
    "        q_table[state, action] = new_value  # Memberikan nilai baru ke q-table\n",
    "        state = next_state\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Training.{'.' * (i%6)}\")\n",
    "        print(f\"Episode: {i}/{total_episodes}\")\n",
    "\n",
    "print(\"\\nTraining finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a8545f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save q_table untuk digunakan di kesempatan berikutnya\n",
    "np.save(\"ModelAgen.npy\", q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d46993f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d965c922",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load q_table\n",
    "q_table = np.load(\"ModelAgen.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ad46e0",
   "metadata": {},
   "source": [
    "## Mengevaluasi performa agen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e72d9222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Learning Agent\n",
      "Results after 100 episodes:\n",
      "Rata-rata timestep per episode: 30.32\n",
      "Rata-rata penalti per episode: 0.0\n",
      "Rata-rata reward per episode : 0.68\n",
      "\n",
      "Random Agent\n",
      "Results after 100 episodes:\n",
      "Rata-rata timestep per episode: 18647.63\n",
      "Rata-rata penalti per episode: 6164.92\n",
      "Rata-rata reward per episode : -74100.91\n"
     ]
    }
   ],
   "source": [
    "#Mengevaluasi performa dari agen\n",
    "\n",
    "results = []\n",
    "total_epochs, total_penalties, total_rewards = 0, 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for eps in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "        total_rewards += reward\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Q-Learning Agent, Current episodes : {eps}\")\n",
    "\n",
    "results.append(\"Q-Learning Agent\")\n",
    "results.append(f\"Results after {episodes} episodes:\")\n",
    "results.append(f\"Rata-rata timestep per episode: {total_epochs / episodes}\")\n",
    "results.append(f\"Rata-rata penalti per episode: {total_penalties / episodes}\")\n",
    "results.append(f\"Rata-rata reward per episode : {total_rewards / episodes}\\n\")\n",
    "\n",
    "# Dibandingkan dengan random agent\n",
    "\n",
    "total_epochs, total_penalties, total_rewards = 0, 0, 0\n",
    "\n",
    "for eps in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "        total_rewards += reward\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Random Agent, Current episodes : {eps}\")\n",
    "\n",
    "\n",
    "results.append(\"Random Agent\")\n",
    "results.append(f\"Results after {episodes} episodes:\")\n",
    "results.append(f\"Rata-rata timestep per episode: {total_epochs / episodes}\")\n",
    "results.append(f\"Rata-rata penalti per episode: {total_penalties / episodes}\")\n",
    "results.append(f\"Rata-rata reward per episode : {total_rewards / episodes}\")\n",
    "\n",
    "clear_output(wait=True)\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ee1612",
   "metadata": {},
   "source": [
    "## Mencoba agen cerdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3bd935f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m+\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m+\u001B[0m\n",
      "\u001B[36m|\u001B[0mR\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : :\u001B[35m\u001B[34;1m\u001B[43mG\u001B[0m\u001B[0m\u001B[0m\u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m \u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : : \u001B[36m|\u001B[0m : : : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0mB\u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m : : \u001B[36m|\u001B[0m : : : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0mY\u001B[36m|\u001B[0m : : : : \u001B[36m|\u001B[0m : : : \u001B[36m|\u001B[0m\n",
      "\u001B[36m+\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m+\u001B[0m\n",
      "  (Taruh Barang)\n",
      "\n",
      "Timestep: 40\n",
      "State: 185\n",
      "Action: 5\n",
      "Reward: 30\n",
      "\n",
      "\n",
      "Final Result\\\n",
      "\tBanyak Langkah yang diambil: 40\n",
      "\tPenalti Terjadi: 0\n"
     ]
    }
   ],
   "source": [
    "# Mencoba agen secara live\n",
    "\n",
    "def print_frames(frames):\n",
    "    \"\"\"Untuk memprint setiap frame dari gameplay, input berupa list berisi dictionary\"\"\"\n",
    "    for i, frame in enumerate(frames):\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.4)\n",
    "        if i != len(frames)-1:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = []  # for animation\n",
    "\n",
    "done = False\n",
    "state = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "    }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "\n",
    "sleep(2)\n",
    "clear_output(wait=True)\n",
    "print_frames(frames)\n",
    "\n",
    "print(f\"\\n\\nFinal Result\\\\\\n\\tBanyak Langkah yang diambil: {epochs}\")\n",
    "print(f\"\\tPenalti Terjadi: {penalties}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2404da0b",
   "metadata": {},
   "source": [
    "# Mencoba mengintip q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ba738d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "q_table_dataframe = pd.DataFrame(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fe8e9d03",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.499993</td>\n",
       "      <td>-2.499988</td>\n",
       "      <td>-2.499988</td>\n",
       "      <td>-2.499988</td>\n",
       "      <td>-2.499980</td>\n",
       "      <td>-11.499988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.429255</td>\n",
       "      <td>-2.382091</td>\n",
       "      <td>-2.382091</td>\n",
       "      <td>-2.382091</td>\n",
       "      <td>-2.303485</td>\n",
       "      <td>-11.382091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.499287</td>\n",
       "      <td>-2.498812</td>\n",
       "      <td>-2.498812</td>\n",
       "      <td>-2.498812</td>\n",
       "      <td>-2.498020</td>\n",
       "      <td>-11.498812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.500000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>-11.394862</td>\n",
       "      <td>-11.489276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>-2.499179</td>\n",
       "      <td>-2.499219</td>\n",
       "      <td>-2.499219</td>\n",
       "      <td>-2.499177</td>\n",
       "      <td>-9.266367</td>\n",
       "      <td>-9.472631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>-2.291657</td>\n",
       "      <td>-2.291775</td>\n",
       "      <td>-2.299338</td>\n",
       "      <td>-2.299960</td>\n",
       "      <td>-4.248313</td>\n",
       "      <td>-3.664688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>-2.496316</td>\n",
       "      <td>-2.496223</td>\n",
       "      <td>-2.496477</td>\n",
       "      <td>-2.496024</td>\n",
       "      <td>-8.464097</td>\n",
       "      <td>-8.764241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-2.031121</td>\n",
       "      <td>-2.029366</td>\n",
       "      <td>-2.027707</td>\n",
       "      <td>-2.029339</td>\n",
       "      <td>-2.802015</td>\n",
       "      <td>-3.664775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3          4          5\n",
       "0     0.000000  0.000000  0.000000  0.000000   0.000000   0.000000\n",
       "1    -2.499993 -2.499988 -2.499988 -2.499988  -2.499980 -11.499988\n",
       "2    -2.429255 -2.382091 -2.382091 -2.382091  -2.303485 -11.382091\n",
       "3    -2.499287 -2.498812 -2.498812 -2.498812  -2.498020 -11.498812\n",
       "4    -2.500000 -2.500000 -2.500000 -2.500000 -11.394862 -11.489276\n",
       "...        ...       ...       ...       ...        ...        ...\n",
       "1995  0.000000  0.000000  0.000000  0.000000   0.000000   0.000000\n",
       "1996 -2.499179 -2.499219 -2.499219 -2.499177  -9.266367  -9.472631\n",
       "1997 -2.291657 -2.291775 -2.299338 -2.299960  -4.248313  -3.664688\n",
       "1998 -2.496316 -2.496223 -2.496477 -2.496024  -8.464097  -8.764241\n",
       "1999 -2.031121 -2.029366 -2.027707 -2.029339  -2.802015  -3.664775\n",
       "\n",
       "[2000 rows x 6 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a27d9627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# misal kita berada di state nomor 1996, \n",
    "#langkah yang akan diambil adalah dengan melihat kolom mana yang memiliki nilai terbesar, yakni :\n",
    "action = np.argmax(q_table[1996])\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a666659c",
   "metadata": {},
   "source": [
    "Dihasilkan nilai-nilai tersebut, beberapa posisi di tabel nilainya 0 semua, hal itu karena tempat tujuan dan tempat barang berasal tidak akan sama. Jadi keadaan ketika lokasi_barang dan tujuan sama tidak pernah terjadi. Disini tetap dibuat seolah2 ada untuk memudahkan encode dan decode keadaan lingkungan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a7d4c8e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robot_row : 0\n",
      "robot_col : 0\n",
      "lokasi barang : 0\n",
      "tujuan : 0\n",
      "\u001B[36m+\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m+\u001B[0m\n",
      "\u001B[36m|\u001B[0m\u001B[35m\u001B[34;1m\u001B[43mR\u001B[0m\u001B[0m\u001B[0m\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : :G\u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m \u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : : \u001B[36m|\u001B[0m : : : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0mB\u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m : : \u001B[36m|\u001B[0m : : : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0m \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m : : : : \u001B[36m|\u001B[0m : \u001B[36m|\u001B[0m\n",
      "\u001B[36m|\u001B[0mY\u001B[36m|\u001B[0m : : : : \u001B[36m|\u001B[0m : : : \u001B[36m|\u001B[0m\n",
      "\u001B[36m+\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m-\u001B[0m\u001B[36m+\u001B[0m\n",
      "  (Taruh Barang)\n",
      "Nilai q-table : [0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# coba kita decode state tersebut\n",
    "robot_row, robot_col, lokasi_barang, tujuan = env.decode(0)\n",
    "print(f\"robot_row : {robot_row}\\nrobot_col : {robot_col}\\nlokasi barang : {lokasi_barang}\\ntujuan : {tujuan}\")\n",
    "env.s = 0\n",
    "env.render()\n",
    "print(f\"Nilai q-table : {q_table[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c975b",
   "metadata": {},
   "source": [
    "# JOB DESK\n",
    "\n",
    "#### Dimas Tri Mustakim (205150200111049)\n",
    "\n",
    "1. Mencari dan mendiskusikan ide\n",
    "2. Membuat program\n",
    "3. Membuat makalah\n",
    "\n",
    "#### Salsabila Dita Prasetya (205150201111024)\n",
    "\n",
    "1. Mendiskusikan ide\n",
    "2. Membuat makalah"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}